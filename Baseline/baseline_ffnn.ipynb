{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Baseline Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, auc, RocCurveDisplay, classification_report\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from scipy.stats import mode\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data = pd.read_csv('syn_data_small.csv', index_col=0)\n",
    "synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(synthetic_data.drop(['output'], axis=1), synthetic_data['output'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 10\n",
    "\n",
    "f1s = []\n",
    "f1_ws = []\n",
    "\n",
    "y_preds = np.zeros((n_runs, len(y_test)))\n",
    "y_probs = np.zeros((n_runs, len(y_test), 3))\n",
    "\n",
    "for i in range(n_runs):\n",
    "    model = MLPClassifier(activation='tanh', hidden_layer_sizes=(100,50,10), max_iter=1000)\n",
    "    model.fit(X_test, y_test)\n",
    "    y_partical_pred = model.predict(X_test)\n",
    "\n",
    "    f1 = round(f1_score(y_test, y_partical_pred, average=\"macro\"), 2)\n",
    "    f1s += [f1]\n",
    "    f1_w = round(f1_score(y_test, y_partical_pred, average=\"weighted\"), 2)\n",
    "    f1_ws += [f1_w]\n",
    "    print(\"Model\", i, f1, \"/\", f1_w)\n",
    "\n",
    "    y_preds[i] = y_partical_pred\n",
    "    y_probs[i] = model.predict_proba(X_test)\n",
    "\n",
    "print(\"AVG\", sum(f1s)/len(f1s), \"/\", sum(f1_ws)/len(f1_ws))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mode(y_pred, axis=0).mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = np.mean(y_probs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred, target_names=[\"Back2Home\", \"Reabilitation\", \"Death\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = round(accuracy_score(y_test, y_pred), 2)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "precision = round(precision_score(y_test, y_pred, average=\"macro\"), 2)\n",
    "precision_w = round(precision_score(y_test, y_pred, average=\"weighted\"), 2)\n",
    "print(\"Precision:\", precision, \"/\", precision_w)\n",
    "recall = round(recall_score(y_test, y_pred, average=\"macro\"), 2)\n",
    "recall_w = round(recall_score(y_test, y_pred, average=\"weighted\"), 2)\n",
    "print(\"Recall:\", recall, \"/\", recall_w)\n",
    "f1_micro = round(f1_score(y_test, y_pred, average=\"micro\"), 2)\n",
    "f1 = round(f1_score(y_test, y_pred, average=\"macro\"), 2)\n",
    "f1_w = round(f1_score(y_test, y_pred, average=\"weighted\"), 2)\n",
    "print(\"F1 Score:\", f1, \"/\", f1_w, \"/\", f1_micro)\n",
    "print(\"F1 per class:\", [round(i, 2) for i in f1_score(y_test, y_pred, average=None)])\n",
    "auc = round(roc_auc_score(y_test, y_score, multi_class='ovr'), 2)\n",
    "print(\"AUC ROC:\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=[\"Back2Home\", \"Reabilitation\", \"Death\"])\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_binarizer = LabelBinarizer().fit(y_train)\n",
    "y_onehot_test = label_binarizer.transform(y_test)\n",
    "y_onehot_test.shape  # (n_samples, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_binarizer.transform([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_of_interest = 0\n",
    "class_id = np.flatnonzero(label_binarizer.classes_ == class_of_interest)[0]\n",
    "class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"y_score:\\n{y_score[0:2,:]}\")\n",
    "print()\n",
    "print(f\"y_score.ravel():\\n{y_score[0:2,:].ravel()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_roc_auc_ovr = roc_auc_score(\n",
    "    y_test,\n",
    "    y_score,\n",
    "    multi_class=\"ovr\",\n",
    "    average=\"micro\",\n",
    ")\n",
    "\n",
    "print(f\"Micro-averaged One-vs-Rest ROC AUC score:\\n{micro_roc_auc_ovr:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the fpr, tpr, and roc_auc for all averaging strategies\n",
    "fpr, tpr, roc_auc = dict(), dict(), dict()\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_onehot_test.ravel(), y_score.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "print(f\"Micro-averaged One-vs-Rest ROC AUC score:\\n{roc_auc['micro']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 3\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_onehot_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "fpr_grid = np.linspace(0.0, 1.0, 1000)\n",
    "\n",
    "# Interpolate all ROC curves at these points\n",
    "mean_tpr = np.zeros_like(fpr_grid)\n",
    "\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(fpr_grid, fpr[i], tpr[i])  # linear interpolation\n",
    "\n",
    "# Average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = fpr_grid\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "print(f\"Macro-averaged One-vs-Rest ROC AUC score:\\n{roc_auc['macro']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro_roc_auc_ovr = roc_auc_score(\n",
    "    y_test,\n",
    "    y_score,\n",
    "    multi_class=\"ovr\",\n",
    "    average=\"macro\",\n",
    ")\n",
    "\n",
    "print(f\"Macro-averaged One-vs-Rest ROC AUC score:\\n{macro_roc_auc_ovr:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = [0.443396, 0.432075, 0.124529]\n",
    "\n",
    "# Compute ROC curve and ROC area for each class with sample weights\n",
    "for i in range(n_classes):\n",
    "    sample_weight = np.ones(y_onehot_test.shape[0]) * class_weights[i]\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_onehot_test[:, i], y_score[:, i], sample_weight=sample_weight)\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Interpolation grid\n",
    "fpr_grid = np.linspace(0.0, 1.0, 1000)\n",
    "\n",
    "# Interpolate all ROC curves at these points\n",
    "weighted_mean_tpr = np.zeros_like(fpr_grid)\n",
    "\n",
    "for i in range(n_classes):\n",
    "    interp_tpr = np.interp(fpr_grid, fpr[i], tpr[i])  # linear interpolation\n",
    "    weighted_mean_tpr += class_weights[i] * interp_tpr\n",
    "\n",
    "# Compute AUC for the weighted macro-average\n",
    "fpr[\"weighted_macro\"] = fpr_grid\n",
    "tpr[\"weighted_macro\"] = weighted_mean_tpr\n",
    "roc_auc[\"weighted_macro\"] = auc(fpr[\"weighted_macro\"], tpr[\"weighted_macro\"])\n",
    "\n",
    "print(f\"Weighted Macro-averaged One-vs-Rest ROC AUC score:\\n{roc_auc['weighted_macro']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "target_names = [\"Back2Home\", \"Reabilitation\", \"Death\"]\n",
    "\n",
    "plt.plot(\n",
    "    fpr[\"macro\"],\n",
    "    tpr[\"macro\"],\n",
    "    label=f\"macro-average (AUC = {roc_auc['macro']:.2f})\",\n",
    "    color=\"navy\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=4,\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    fpr[\"weighted_macro\"],\n",
    "    tpr[\"weighted_macro\"],\n",
    "    label=f\"weighted-average (AUC = {roc_auc['weighted_macro']:.2f})\",\n",
    "    color=\"deeppink\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=4,\n",
    ")\n",
    "\n",
    "colors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\"])\n",
    "for class_id, color in zip(range(n_classes), colors):\n",
    "    RocCurveDisplay.from_predictions(\n",
    "        y_onehot_test[:, class_id],\n",
    "        y_score[:, class_id],\n",
    "        name=f\"{target_names[class_id]}\",\n",
    "        color=color,\n",
    "        ax=ax,\n",
    "        plot_chance_level=(class_id == 2),\n",
    "    )\n",
    "\n",
    "_ = ax.set(\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    "    title=\"Feedforward Neural Network\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    fpr[\"weighted_macro\"],\n",
    "    tpr[\"weighted_macro\"],\n",
    "    label=f\"weighted-average (AUC = {roc_auc['weighted_macro']:.2f})\",\n",
    "    color=\"deeppink\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
